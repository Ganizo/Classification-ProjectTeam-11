{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce49e031",
   "metadata": {},
   "source": [
    "# Analysing News Articles Dataset\n",
    "\n",
    "<div id=\"main image\" align=\"center\">\n",
    "  <img src=\"https://github.com/Ganizo/Classification-ProjectTeam-11/blob/main/Images/newspaper-with-the-headline-news-and-glasses-and-coffee-cup-on-wooden-table-daily-newspaper-mock-up-concept-photo.jpg\" width=\"550\" height=\"300\" alt=\"\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e698956",
   "metadata": {},
   "source": [
    "\n",
    "## Structure of the Notebook\n",
    "* 1. Importing Pacakages\n",
    "* 2. Dataset\n",
    "* 3. Pre-Processing\n",
    "* 4. DataPrep\n",
    "* 5. Model train and test\n",
    "* 6. Model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9ac31c",
   "metadata": {},
   "source": [
    "## 1. Importing Pacakages <a class=\"anchor\" id=\"packages\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c94b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Packages\n",
    "\n",
    "# Importing Pandas for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Importing NumPy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Importing Matplotlib for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing Seaborn for advanced data visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing IPython.display for displaying rich media in Jupyter Notebooks\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Importing IPython.display for displaying rich media in Jupyter Notebooks\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Importing models used for training\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Importing model metrics to evaluate performance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "\n",
    "#Importing Pickle to save and use the model\n",
    "import pickle\n",
    "\n",
    "#Importing re\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3021798b",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________________\n",
    "\n",
    "############################################################################\n",
    "__________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f308e493",
   "metadata": {},
   "source": [
    "## 2. Data Loading <a class=\"anchor\" id=\"dataset\"></a>\n",
    "    \n",
    "    Firstly, import the necessary packages. When the required packages are imported, for loading a CSV file, this Project will utilise Pandas to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f903510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "train_df = pd.read_csv('Data/processed/train.csv')\n",
    "test_df = pd.read_csv('Data/processed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ce348d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NLC India wins contract for power supply to Ra...</td>\n",
       "      <td>State-owned firm NLC India Ltd (NLCIL) on Mond...</td>\n",
       "      <td>State-owned firm NLC India Ltd (NLCIL) on Mond...</td>\n",
       "      <td>https://indianexpress.com/article/business/com...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SBI Clerk prelims exams dates announced; admit...</td>\n",
       "      <td>SBI Clerk Prelims Exam: The SBI Clerk prelims ...</td>\n",
       "      <td>SBI Clerk Prelims Exam: The State Bank of Indi...</td>\n",
       "      <td>https://indianexpress.com/article/education/sb...</td>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Golden Globes: Michelle Yeoh, Will Ferrell, An...</td>\n",
       "      <td>Barbie is the top nominee this year, followed ...</td>\n",
       "      <td>Michelle Yeoh, Will Ferrell, Angela Bassett an...</td>\n",
       "      <td>https://indianexpress.com/article/entertainmen...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OnePlus Nord 3 at Rs 27,999 as part of new pri...</td>\n",
       "      <td>New deal makes the OnePlus Nord 3 an easy purc...</td>\n",
       "      <td>In our review of the OnePlus Nord 3 5G, we pra...</td>\n",
       "      <td>https://indianexpress.com/article/technology/t...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adani family’s partners used ‘opaque’ funds to...</td>\n",
       "      <td>Citing review of files from multiple tax haven...</td>\n",
       "      <td>Millions of dollars were invested in some publ...</td>\n",
       "      <td>https://indianexpress.com/article/business/ada...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  NLC India wins contract for power supply to Ra...   \n",
       "1  SBI Clerk prelims exams dates announced; admit...   \n",
       "2  Golden Globes: Michelle Yeoh, Will Ferrell, An...   \n",
       "3  OnePlus Nord 3 at Rs 27,999 as part of new pri...   \n",
       "4  Adani family’s partners used ‘opaque’ funds to...   \n",
       "\n",
       "                                         description  \\\n",
       "0  State-owned firm NLC India Ltd (NLCIL) on Mond...   \n",
       "1  SBI Clerk Prelims Exam: The SBI Clerk prelims ...   \n",
       "2  Barbie is the top nominee this year, followed ...   \n",
       "3  New deal makes the OnePlus Nord 3 an easy purc...   \n",
       "4  Citing review of files from multiple tax haven...   \n",
       "\n",
       "                                             content  \\\n",
       "0  State-owned firm NLC India Ltd (NLCIL) on Mond...   \n",
       "1  SBI Clerk Prelims Exam: The State Bank of Indi...   \n",
       "2  Michelle Yeoh, Will Ferrell, Angela Bassett an...   \n",
       "3  In our review of the OnePlus Nord 3 5G, we pra...   \n",
       "4  Millions of dollars were invested in some publ...   \n",
       "\n",
       "                                                 url       category  \n",
       "0  https://indianexpress.com/article/business/com...       business  \n",
       "1  https://indianexpress.com/article/education/sb...      education  \n",
       "2  https://indianexpress.com/article/entertainmen...  entertainment  \n",
       "3  https://indianexpress.com/article/technology/t...     technology  \n",
       "4  https://indianexpress.com/article/business/ada...       business  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b85270e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RBI revises definition of politically-exposed ...</td>\n",
       "      <td>The central bank has also asked chairpersons a...</td>\n",
       "      <td>The Reserve Bank of India (RBI) has changed th...</td>\n",
       "      <td>https://indianexpress.com/article/business/ban...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDTV Q2 net profit falls 57.4% to Rs 5.55 cror...</td>\n",
       "      <td>NDTV's consolidated revenue from operations wa...</td>\n",
       "      <td>Broadcaster New Delhi Television Ltd on Monday...</td>\n",
       "      <td>https://indianexpress.com/article/business/com...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akasa Air ‘well capitalised’, can grow much fa...</td>\n",
       "      <td>The initial share sale will be open for public...</td>\n",
       "      <td>Homegrown server maker Netweb Technologies Ind...</td>\n",
       "      <td>https://indianexpress.com/article/business/mar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India’s current account deficit declines sharp...</td>\n",
       "      <td>The current account deficit (CAD) was 3.8 per ...</td>\n",
       "      <td>India’s current account deficit declined sharp...</td>\n",
       "      <td>https://indianexpress.com/article/business/eco...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>States borrowing cost soars to 7.68%, highest ...</td>\n",
       "      <td>The prices shot up reflecting the overall high...</td>\n",
       "      <td>States have been forced to pay through their n...</td>\n",
       "      <td>https://indianexpress.com/article/business/eco...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  RBI revises definition of politically-exposed ...   \n",
       "1  NDTV Q2 net profit falls 57.4% to Rs 5.55 cror...   \n",
       "2  Akasa Air ‘well capitalised’, can grow much fa...   \n",
       "3  India’s current account deficit declines sharp...   \n",
       "4  States borrowing cost soars to 7.68%, highest ...   \n",
       "\n",
       "                                         description  \\\n",
       "0  The central bank has also asked chairpersons a...   \n",
       "1  NDTV's consolidated revenue from operations wa...   \n",
       "2  The initial share sale will be open for public...   \n",
       "3  The current account deficit (CAD) was 3.8 per ...   \n",
       "4  The prices shot up reflecting the overall high...   \n",
       "\n",
       "                                             content  \\\n",
       "0  The Reserve Bank of India (RBI) has changed th...   \n",
       "1  Broadcaster New Delhi Television Ltd on Monday...   \n",
       "2  Homegrown server maker Netweb Technologies Ind...   \n",
       "3  India’s current account deficit declined sharp...   \n",
       "4  States have been forced to pay through their n...   \n",
       "\n",
       "                                                 url  category  \n",
       "0  https://indianexpress.com/article/business/ban...  business  \n",
       "1  https://indianexpress.com/article/business/com...  business  \n",
       "2  https://indianexpress.com/article/business/mar...  business  \n",
       "3  https://indianexpress.com/article/business/eco...  business  \n",
       "4  https://indianexpress.com/article/business/eco...  business  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0774de8",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________________\n",
    "\n",
    "############################################################################\n",
    "__________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62057f78",
   "metadata": {},
   "source": [
    "## 3. Pre-Processing <a class=\"anchor\" id=\"preprocess\"></a>\n",
    "\n",
    "- Remove unnecessary columns (url) since it may not contribute to categorization.\n",
    "- *Text Cleaning*: Convert text to lowercase, remove punctuation, stopwords, and special characters.\n",
    "- *Tokenization*: Split text into individual words or phrases.\n",
    "- *Vectorization*: Convert text into numerical form using TF-IDF or CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c929bc",
   "metadata": {},
   "source": [
    "After analysising my data We have decided to reduce my dataset to the headlines and Category tabs, Reason being Describtion and content usually contain alot more verbose explaination of the headlines, meaning we need to use alot more resources to process,train and test our model.\n",
    "\n",
    "Headlines are similar to article titles meaning chances are can get a pretty good model at a fraction of the resource. This is our initial hypothesis.\n",
    "\n",
    "As a result we will drop the **url**, **Describtion** and **content** columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26df05b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RBI revises definition of politically-exposed ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDTV Q2 net profit falls 57.4% to Rs 5.55 cror...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akasa Air ‘well capitalised’, can grow much fa...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India’s current account deficit declines sharp...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>States borrowing cost soars to 7.68%, highest ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  category\n",
       "0  RBI revises definition of politically-exposed ...  business\n",
       "1  NDTV Q2 net profit falls 57.4% to Rs 5.55 cror...  business\n",
       "2  Akasa Air ‘well capitalised’, can grow much fa...  business\n",
       "3  India’s current account deficit declines sharp...  business\n",
       "4  States borrowing cost soars to 7.68%, highest ...  business"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove unnecessary columns\n",
    "train_df = train_df.drop('url', axis=1)\n",
    "train_df = train_df.drop('description', axis=1)\n",
    "train_df = train_df.drop('content', axis=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca213796",
   "metadata": {},
   "source": [
    "Next up we need to remove the noise within the headlines column, so we will make use of regex to remove punctuation, special characters and numbers. Afterwards we will use nltk to download and set stop words array then we use a for loop to cycle through our text to remove any stop words.\n",
    "\n",
    "Lastly we make sure to make all our text lower case to make it uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db3ea814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>cleaned_headlines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>rbi revises definition politically exposed per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>ndtv q net profit falls rs crore impacted lowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>akasa air well capitalised grow much faster ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>india current account deficit declines sharply...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>states borrowing cost soars highest far fiscal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                  cleaned_headlines\n",
       "0  business  rbi revises definition politically exposed per...\n",
       "1  business  ndtv q net profit falls rs crore impacted lowe...\n",
       "2  business  akasa air well capitalised grow much faster ce...\n",
       "3  business  india current account deficit declines sharply...\n",
       "4  business     states borrowing cost soars highest far fiscal"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text Cleaning\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words\n",
    "def clean_text(text):\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Remove numbers from the text\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Apply the above function to the headings column\n",
    "train_df['cleaned_headlines'] = train_df['headlines'].apply(clean_text)\n",
    "\n",
    "\n",
    "# Make the entire column lowercase\n",
    "train_df['cleaned_headlines'] = train_df['cleaned_headlines'].str.lower()\n",
    "\n",
    "\n",
    "train_df = train_df.drop('headlines', axis=1)\n",
    "\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba2ff1",
   "metadata": {},
   "source": [
    "Down here we tried using varies sort of tokenizers, vectorizers and we ended up using TD-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1280c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenise the text using the TreebankWordTokenizer\n",
    "#tokeniser = TreebankWordTokenizer()\n",
    "#train_df['tokens_content'] = train_df['cleaned_content'].apply(tokeniser.tokenize)\n",
    "#train_df['tokens_description'] = train_df['cleaned_description'].apply(tokeniser.tokenize)\n",
    "#train_df['tokens_headlines'] = train_df['cleaned_headlines'].apply(tokeniser.tokenize)\n",
    "\n",
    "#train_df = train_df.drop('cleaned_content', axis=1)\n",
    "#train_df = train_df.drop('cleaned_description', axis=1)\n",
    "#train_df = train_df.drop('cleaned_headlines', axis=1)\n",
    "\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c20bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialise CountVectorizer\n",
    "#vect = CountVectorizer()\n",
    "# Fit the CountVectorizer on the preprocessed 'post' column\n",
    "#vect.fit(train_df['cleaned_content'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d138d",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________________\n",
    "\n",
    "############################################################################\n",
    "__________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af69550",
   "metadata": {},
   "source": [
    "## 4. DataPrep <a class=\"anchor\" id=\"dataprep\"></a>\n",
    "\n",
    "So below we are just seperating our features and labels to be sent into our models. We were not sure if we needed to use the train test split since our dataset came separated already so ultimately we decide to just go with using as we are comfortable training and testing our models this way. \n",
    "\n",
    "Due to time contraints we would have expolre this further but for the train dataset and the test dataset we did the train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7705e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(train_df[\"cleaned_headlines\"]) \n",
    "\n",
    "# Setting up our labels\n",
    "y = train_df[\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f67c8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e8d770",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________________\n",
    "\n",
    "############################################################################\n",
    "__________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a91bf",
   "metadata": {},
   "source": [
    "## 5. Model Train and Test <a class=\"anchor\" id=\"modeltandt\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27fd1590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.9039855072463768\n",
      "SVM Accuracy: 0.9320652173913043\n",
      "LogisticRegression Accuracy: 0.9193840579710145\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Save the trained model\n",
    "#train LogisticRegression \n",
    "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "with open(\"Streamlit/LogisticRegression.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lr, f)\n",
    "\n",
    "# Train Naive Bayes and save for streamlit usage\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb_pred = nb.predict(X_test)\n",
    "with open(\"Streamlit/Naive_Bayes.pkl\", \"wb\") as f:\n",
    "    pickle.dump(nb, f)\n",
    "\n",
    "# Train SVM and save for streamlit usage\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X_train, y_train)\n",
    "svm_pred = svm.predict(X_test)\n",
    "with open(\"Streamlit/SVM.pkl\", \"wb\") as f:\n",
    "    pickle.dump(svm, f)\n",
    "\n",
    "# Save Vectorizer for streamlit usage\n",
    "with open(\"Streamlit/vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "# Evaluate Models\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, nb_pred))\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
    "print(\"LogisticRegression Accuracy:\", accuracy_score(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840659f4",
   "metadata": {},
   "source": [
    "Test trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a138f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df['cleaned_headlines'] = test_df['headlines'].apply(clean_text)\n",
    "\n",
    "\n",
    "# Make the entire column lowercase\n",
    "test_df['cleaned_headlines'] = test_df['cleaned_headlines'].str.lower()\n",
    "\n",
    "\n",
    "test_df = test_df.drop('content', axis=1)\n",
    "test_df = test_df.drop('description', axis=1)\n",
    "test_df = test_df.drop('headlines', axis=1)\n",
    "\n",
    "#vectorizer = TfidfVectorizer()\n",
    "X_test_vectorized = vectorizer.transform(test_df[\"cleaned_headlines\"]) \n",
    "y_test = test_df[\"category\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf561a",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________________\n",
    "\n",
    "############################################################################\n",
    "__________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ce844",
   "metadata": {},
   "source": [
    "## 6. Model Performance <a class=\"anchor\" id=\"modelperform\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df75853",
   "metadata": {},
   "source": [
    "Checking Performance of models on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15e78d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Accuracy:\n",
      "Accuracy: 0.9075\n",
      "Precision: 0.9133\n",
      "Recall: 0.9075\n",
      "F1 Score: 0.9072\n",
      "AUC-ROC: 0.9920\n",
      "Log Loss: 0.5550\n",
      "[[67  1  1  0  4]\n",
      " [ 0 78  0  0  2]\n",
      " [ 1  0 75  1  4]\n",
      " [ 5  2  5 68  6]\n",
      " [ 4  1  0  0 75]]\n",
      "------------------------------------------\n",
      "******************************************\n",
      "------------------------------------------\n",
      "Naive Bayes Accuracy:\n",
      "Accuracy: 0.8850\n",
      "Precision: 0.8950\n",
      "Recall: 0.8850\n",
      "F1 Score: 0.8821\n",
      "AUC-ROC: 0.9935\n",
      "Log Loss: 0.5652\n",
      "[[66  4  0  0  3]\n",
      " [ 0 78  0  0  2]\n",
      " [ 0  2 75  0  4]\n",
      " [ 7  5  8 59  7]\n",
      " [ 3  1  0  0 76]]\n",
      "------------------------------------------\n",
      "******************************************\n",
      "------------------------------------------\n",
      "SVM Accuracy:\n",
      "Accuracy: 0.9300\n",
      "Precision: 0.9320\n",
      "Recall: 0.9300\n",
      "F1 Score: 0.9301\n",
      "[[68  1  1  0  3]\n",
      " [ 0 78  0  0  2]\n",
      " [ 1  0 76  1  3]\n",
      " [ 3  0  5 75  3]\n",
      " [ 3  1  0  1 75]]\n"
     ]
    }
   ],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_test_vectorized, y_test, test_size=0.2, random_state=42)\n",
    "\n",
    "#performing model predictions\n",
    "lr_pred2 = lr.predict(X_test2)\n",
    "nb_pred2 = nb.predict(X_test2)\n",
    "svm_pred2= svm.predict(X_test2)\n",
    "\n",
    "#LogisticRegression metrics\n",
    "lr_cm = confusion_matrix(y_test2, lr_pred2)\n",
    "#lr_tn, lr_fp, lr_fn, lr_tp = lr_cm.ravel()\n",
    "lr_accuracy = accuracy_score(y_test2, lr_pred2)\n",
    "lr_precision = precision_score(y_test2, lr_pred2, average='weighted')\n",
    "lr_recall = recall_score(y_test2, lr_pred2, average='weighted')\n",
    "lr_f1 = f1_score(y_test2, lr_pred2, average='weighted')\n",
    "lr_auc_roc = roc_auc_score(y_test2, lr.predict_proba(X_test2), multi_class='ovr')\n",
    "lr_log_loss_val = log_loss(y_test2, lr.predict_proba(X_test2))\n",
    "\n",
    "#Naive bayes Metrics\n",
    "nb_cm = confusion_matrix(y_test2, nb_pred2)\n",
    "#nb_tn, nb_fp, nb_fn, nb_tp = nb_cm.ravel()\n",
    "nb_accuracy = accuracy_score(y_test2, nb_pred2)\n",
    "nb_precision = precision_score(y_test2, nb_pred2, average='weighted')\n",
    "nb_recall = recall_score(y_test2, nb_pred2, average='weighted')\n",
    "nb_f1 = f1_score(y_test2, nb_pred2, average='weighted')\n",
    "nb_auc_roc = roc_auc_score(y_test2, nb.predict_proba(X_test2), multi_class='ovr')\n",
    "nb_log_loss_val = log_loss(y_test2, nb.predict_proba(X_test2))\n",
    "\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test2, svm_pred2)\n",
    "svm_precision = precision_score(y_test2, svm_pred2, average='weighted')\n",
    "svm_recall = recall_score(y_test2, svm_pred2, average='weighted')\n",
    "svm_f1 = f1_score(y_test2, svm_pred2, average='weighted')\n",
    "svm_cm = confusion_matrix(y_test2, svm_pred2)\n",
    "#svm_tn, svm_fp, svm_fn, svm_tp = svm_cm.ravel()\n",
    "#svm_auc_roc = roc_auc_score(y_test2, svm.predict_proba(X_test2), multi_class='ovr')\n",
    "#svm_log_loss_val = log_loss(y_test2, svm.predict_proba(X_test2))\n",
    "\n",
    "print(\"LogisticRegression Accuracy:\")\n",
    "print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"Precision: {lr_precision:.4f}\")\n",
    "print(f\"Recall: {lr_recall:.4f}\")\n",
    "print(f\"F1 Score: {lr_f1:.4f}\")\n",
    "print(f\"AUC-ROC: {lr_auc_roc:.4f}\")\n",
    "print(f\"Log Loss: {lr_log_loss_val:.4f}\")\n",
    "print(lr_cm)\n",
    "print(\"------------------------------------------\")\n",
    "print(\"******************************************\")\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Naive Bayes Accuracy:\")\n",
    "print(f\"Accuracy: {nb_accuracy:.4f}\")\n",
    "print(f\"Precision: {nb_precision:.4f}\")\n",
    "print(f\"Recall: {nb_recall:.4f}\")\n",
    "print(f\"F1 Score: {nb_f1:.4f}\")\n",
    "print(f\"AUC-ROC: {nb_auc_roc:.4f}\")\n",
    "print(f\"Log Loss: {nb_log_loss_val:.4f}\")\n",
    "print(nb_cm)\n",
    "#print(f\"confusion_matrix: {nb_cm:.4f}\")\n",
    "print(\"------------------------------------------\")\n",
    "print(\"******************************************\")\n",
    "print(\"------------------------------------------\")\n",
    "print(\"SVM Accuracy:\")\n",
    "print(f\"Accuracy: {svm_accuracy:.4f}\")\n",
    "print(f\"Precision: {svm_precision:.4f}\")\n",
    "print(f\"Recall: {svm_recall:.4f}\")\n",
    "print(f\"F1 Score: {svm_f1:.4f}\")\n",
    "print(svm_cm)\n",
    "#print(f\"AUC-ROC: {svm_auc_roc:.4f}\")\n",
    "#print(f\"Log Loss: {svm_log_loss_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce5584",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________________\n",
    "\n",
    "############################################################################\n",
    "__________________________________________________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
